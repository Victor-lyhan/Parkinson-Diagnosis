{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Victor-lyhan/Parkinson-Diagnosis/blob/main/nlp_Parkinson's_Diagnose.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "styUeZPaHF32"
      },
      "source": [
        "# NLP - Tests\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKCczpbOHOwl"
      },
      "source": [
        "## Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEWLzmusGtrZ",
        "outputId": "acc4b7ff-c7b6-4e63-d6e7-a773b25f33d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download()\n",
        "\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "\n",
        "text = \"Computer science is the study of computation, automation, and information.\" \n",
        "\"Computer science spans theoretical disciplines (such as algorithms, theory of computation, information theory, and automation) to practical disciplines (including the design and implementation of hardware and software).\" \n",
        "\"Computer science is generally considered an academic discipline and distinct from computer programming\"\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhhprJduJvMx",
        "outputId": "aea800d0-a681-47d5-ff62-6aa88ca54dd2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Computer science is the study of computation, automation, and information.']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sent_tokenize(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLMyAp4wKGC0"
      },
      "source": [
        "## Filter Stop Words\n",
        "\n",
        "I love you\n",
        "\n",
        "I hate you"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocJEnI1xKP1E",
        "outputId": "17fdbb7d-5234-43c7-dc90-697765bcb134"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'through', \"hasn't\", 'other', 'hasn', 'his', 'if', 'me', 'again', 'only', \"she's\", 'm', \"shan't\", 'by', 'most', 'this', 'in', 'themselves', 'was', 'own', 'any', 'll', 'while', \"didn't\", \"weren't\", 'when', 'will', 'that', 'doing', \"you've\", 'herself', 'aren', 'but', 'won', 'my', 'it', 'on', 'do', 'shan', 'i', \"aren't\", 'down', 'ours', 'and', 'after', 'mustn', 'for', 'nor', 'yourself', 'whom', 'are', 'these', 'your', \"don't\", 'does', \"wasn't\", 'having', 'some', 'from', \"wouldn't\", 've', 'what', 'had', 't', 'during', 'too', 'be', 'y', 'you', 'which', 'over', 'she', 'their', 'just', 'there', 'did', 'how', \"that'll\", 'not', 'been', 'don', 's', 'the', 'each', 'her', 'few', 'have', 'they', 'can', 'up', 'at', 'before', 're', 'them', 'or', 'an', 'its', 'am', 'ain', \"mustn't\", 'o', 'now', 'mightn', 'of', 'here', 'our', 'off', 'between', 'with', 'then', 'why', 'him', 'couldn', 'no', 'all', 'very', 'because', 'isn', 'above', \"isn't\", 'doesn', 'wasn', 'theirs', \"hadn't\", 'same', 'weren', 'should', 'both', 'so', 'such', 'wouldn', 'didn', 'against', 'where', 'haven', 'has', 'yourselves', 'more', 'into', 'being', 'further', \"should've\", 'were', \"it's\", 'we', 'those', 'd', 'needn', \"won't\", 'under', 'who', \"mightn't\", 'below', 'once', 'ma', 'yours', 'to', \"doesn't\", 'shouldn', \"couldn't\", 'than', 'out', 'until', \"needn't\", 'about', 'hers', \"you'll\", 'is', 'a', \"you'd\", 'ourselves', 'he', 'as', 'itself', 'myself', \"shouldn't\", \"you're\", 'hadn', 'himself', \"haven't\"}\n",
            "['Computer', 'science', 'study', 'computation', ',', 'automation', ',', 'information', '.']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/hanliyang/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "print(stop_words)\n",
        "\n",
        "filter_words = []\n",
        "for t in tokens:\n",
        "  if t not in stop_words:\n",
        "    filter_words.append(t)\n",
        "\n",
        "print(filter_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "houK7EhiMw_N"
      },
      "source": [
        "## Stem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HYL_kwZNhbX",
        "outputId": "78b81a96-9f9a-46d1-9401-a26feef552ad"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /Users/hanliyang/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['cat', 'cactus', 'goose', 'goose']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_words(words):\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_words.append(lemmatizer.lemmatize(word))\n",
        "    return new_words\n",
        "\n",
        "lemmatize_words([\"cats\", \"cacti\", \"geese\", 'goose'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGDCjCD27110"
      },
      "source": [
        "# Parkinson's Diagnose"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Audio Conversion\n",
        "* Using assembly AI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I'm David Curley at the Smithsonian Air and Space Museum, where we are marking 50 years since man landed and walked on the moon in a lander just like this one. We are going to show you some of the actual ABC News coverage from 50 years ago. During that eight day mission of this remarkable achievement, Apollo Eleven's lander, the Eagle, would be the first man craft to land on the moon. For training, NASA came up with an unusual contraption. Neil Armstrong actually had to eject from it once, and then he had a couple of successful flights. ABC News anchor at the time, 50 years ago, Frank Reynolds with a look at that unusual trainer. Apollo Eleven Commander Neil Armstrong is at the controls of a lunar landing training vehicle, testing the reaction control jets. These thrusters stabilize the LEM during landing and takeoff. The LLTV is designed to simulate the behavior of the LEM as it lands in the moon's gravity. Lunar gravity is one 6th that of the Earth's. Neil ArmstroNG flew one of these vehicles on May 6, 1968, and that flight was nearly his last. Later reports by a NASA investigating team said the crash, which we'll see soon, was caused by a loss of fuel pressure compounded by a warning light that failed to work. Armstrong's coolness under pressure saved himself and possibly months of delay for the Apollo program. Later that same year, 1968, another test pilot, Joseph Allegranti, also escaped from an LLTV just before it crashed. The training vehicle then underwent several design modifications and improvements. Engineers had to increase the vehicle's rocket power to help stabilize the craft. That made the LLTV less of a moon gravity simulator, but it improved pilot safety. On June 16, 1969, Neil Armstrong flew the vehicle, sometimes called the flying bedstead, to several perfect landings. The question was, how does the machine fly? And the answer is that we're very pleased with the way it flies. It's a significant improvement over the LLRV, which we were flying here a year ago, and I think it does an excellent job of actually capturing the handling characteristics of the lunar module in landing maneuver. It's really a great deal different than any other kind of aircraft that I've ever flown. The simulation of lunar gravity has some aspects that make this type of flight sufficiently different from anything else we've ever done to make this vehicle very worthwhile. And I'm very pleased that I had the opportunity to get some flights in it here just before the Apollo eleven flight.\n"
          ]
        }
      ],
      "source": [
        "import assemblyai as aai\n",
        "\n",
        "aai.settings.api_key = \"d9ccd193e5e54a4ca8bdddaf3769bc59\"\n",
        "transcriber = aai.Transcriber()\n",
        "\n",
        "transcript = transcriber.transcribe(\"https://storage.googleapis.com/aai-web-samples/news.mp4\")\n",
        "\n",
        "print(transcript.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wQQ03Ow8u6J"
      },
      "source": [
        "## counting pos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPsS5RBY9RjC",
        "outputId": "2995c83c-e99a-4c43-aafe-e1e90945d481"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /Users/hanliyang/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /Users/hanliyang/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "tokens = \"Computer science is the study of computation, automation, and information.\"\\\n",
        "\"Computer science spans theoretical disciplines (such as algorithms, theory of computation, information theory, and automation) to practical disciplines (including the design and implementation of hardware and software).\"\\\n",
        "\"Computer science is generally considered an academic discipline and distinct from computer programming.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1v58QMikBnZc"
      },
      "source": [
        "* verbs - VB\tverb (ask)\n",
        "VBG\tverb gerund (judging)\n",
        "VBD\tverb past tense (pleaded)\n",
        "VBN\tverb past participle (reunified)\n",
        "VBP\tverb, present tense not 3rd person singular(wrap)\n",
        "VBZ\tverb, present tense with 3rd person singular (bases)\n",
        "* common nouns - NN, NNS\n",
        "* proper nouns - NNP, NNPS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "O2VPwr9A-OET"
      },
      "outputs": [],
      "source": [
        "filler_words = [\n",
        "    \"um\", \"uh\", \"er\", \"like\", \"you know\", \"i mean\", \"well\", \"so\", \"okay\", \"right\",\n",
        "    \"actually\", \"basically\", \"literally\", \"kind of\", \"sort of\", \"anyway\", \"or something\",\n",
        "    \"whatever\", \"i guess\", \"you see\", \"got it\", \"i suppose\", \"just\", \"yeah\", \"yep\",\n",
        "    \"seriously\", \"alright\", \"hmm\", \"look\", \"totally\", \"alright then\", \"know what i mean?\",\n",
        "    \"at the end of the day\", \"to be honest\", \"if you know what i mean\"\n",
        "]\n",
        "\n",
        "def pdFeaturesCount(tokens):\n",
        "  wordList = pos_tag(word_tokenize(tokens))\n",
        "  totalWords, verbsRate, verbs, commonNouns, properNouns, fillers = 0, 0, 0, 0, 0, 0\n",
        "  for word, pos in wordList:\n",
        "    #print(word, pos)\n",
        "    totalWords += 1\n",
        "    if pos in ['VB', 'VBG', 'VBD', 'VBN', 'VBP', 'VBZ']:\n",
        "      verbs += 1\n",
        "    if pos in ['NN', 'NNS']:\n",
        "      commonNouns += 1\n",
        "    if pos in ['NNP', 'NNPS']:\n",
        "      properNouns += 1\n",
        "    if word in filler_words:\n",
        "      fillers += 1\n",
        "  verbsRate = verbs / totalWords\n",
        "  return totalWords, verbsRate, verbs, commonNouns, properNouns, fillers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYThN75NAFt4",
        "outputId": "363b1c18-bb7a-4b46-dee3-7de5b1e6fa3a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(59, 0.06779661016949153, 4, 25, 1, 0)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pdFeaturesCount(tokens)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNORmJtNOZTaXoell/LOQTl",
      "collapsed_sections": [
        "PKCczpbOHOwl",
        "KLMyAp4wKGC0"
      ],
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
